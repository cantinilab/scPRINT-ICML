{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m→\u001b[0m connected lamindb: jkobject/scprint2\n"
     ]
    }
   ],
   "source": [
    "! lamin load jkobject/scprint2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m→\u001b[0m connected lamindb: jkobject/scprint2\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, StochasticWeightAveraging, EarlyStopping, LearningRateMonitor, LearningRateFinder\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "\n",
    "from scprint import scPrint\n",
    "from scprint.trainer import TrainingMode\n",
    "from scdataloader import DataModule \n",
    "import pandas as pd\n",
    "from scdataloader.utils import load_genes\n",
    "import lamindb as ln\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: drop tissue & dev stage until part or is taken in account\n",
    "\n",
    "hierarchical_clss = [\n",
    "    \"cell_type_ontology_term_id\",  # 1\n",
    "    \"tissue_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\",  # 2\n",
    "    \"simplified_dev_stage\",\n",
    "    \"assay_ontology_term_id\",  # 3\n",
    "    'self_reported_ethnicity_ontology_term_id',  # 4\n",
    "]\n",
    "clss_to_predict = hierarchical_clss+[\n",
    "    'sex_ontology_term_id',  # 5\n",
    "    \"organism_ontology_term_id\",  # 6\n",
    "    \"cell_culture\"\n",
    "]\n",
    "clss_to_weight = [\n",
    "    \"tissue_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    \"simplified_dev_stage\",\n",
    "    \"assay_ontology_term_id\",\n",
    "    \"organism_ontology_term_id\",\n",
    "    \"clust_cell_type\",\n",
    "    # 'dataset_id',\n",
    "    # 'cell_culture',\n",
    "    #  \"heat_diff\",\n",
    "    #  \"total_counts\",\n",
    "    \"nnz\",\n",
    "    #  \"dpt_group\",\n",
    "]\n",
    "\n",
    "gene_emb = '../data/main/gene_embeddings.parquet'\n",
    "d_model = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m!\u001b[0m no run & transform got linked, call `ln.track()` & re-run\n",
      "\u001b[93m!\u001b[0m run input wasn't tracked, call `ln.track()` and re-run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "won't do any check but we recommend to have your dataset coming from local storage\n",
      "100.0% are aligned\n",
      "seeing a string: loading gene positions as biomart parquet file\n"
     ]
    }
   ],
   "source": [
    "datamodule = DataModule(\n",
    "    collection_name=\"scPRINT-V2 test\", #some, all, preprocessed dataset, all no zhang, \n",
    "    gene_embeddings=gene_emb,\n",
    "    clss_to_weight=clss_to_weight,\n",
    "    metacell_mode=0.2,\n",
    "    clss_to_predict=clss_to_predict,\n",
    "    hierarchical_clss=hierarchical_clss,\n",
    "    organisms=[\"NCBITaxon:9606\"],#, \"NCBITaxon:10090\"],\n",
    "    how=\"random expr\",\n",
    "    max_len=3200,\n",
    "    add_zero_genes=0,\n",
    "    # how much more you will see the most present vs less present category\n",
    "    weight_scaler=100,\n",
    "    batch_size=8,\n",
    "    num_workers=20,\n",
    "    prefetch_factor=3,\n",
    "    train_oversampling_per_epoch=2,\n",
    "    validation_split=0.05,\n",
    "    do_gene_pos='../data/main/biomart_pos.parquet',\n",
    "    pin_memory=True,\n",
    "    test_split=0.05)\n",
    "testfiles = datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "model = scPrint(\n",
    "    genes=datamodule.genes,\n",
    "    d_model=d_model*2,\n",
    "    nhead=2*2,\n",
    "    num_heads_kv=2,\n",
    "    nlayers=8,\n",
    "    layers_cls = [d_model],\n",
    "    classes = datamodule.classes,\n",
    "    labels_hierarchy = datamodule.labels_hierarchy,\n",
    "    dropout=0.1,\n",
    "    transformer=\"flash\",\n",
    "    precpt_gene_emb=gene_emb,\n",
    "    gene_pos_enc=datamodule.gene_pos,\n",
    "    mvc_decoder=\"inner product\",\n",
    "    label_decoders = datamodule.decoders,\n",
    "    num_batch_labels = datamodule.num_datasets,\n",
    "    checkpointing=True,\n",
    "    prenorm=True,\n",
    "    cell_specific_blocks=True,\n",
    "    #weight_decay=0.01,\n",
    "    #zinb=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0382,  0.0023, -0.0086,  ...,  0.0048,  0.0066, -0.0260],\n",
       "        [-0.0003, -0.0039,  0.0257,  ...,  0.0108, -0.0125,  0.0233],\n",
       "        [-0.0002,  0.0473,  0.0204,  ..., -0.0133,  0.0038,  0.0198],\n",
       "        ...,\n",
       "        [ 0.0330,  0.0147, -0.0138,  ...,  0.0136,  0.0110,  0.0110],\n",
       "        [ 0.0221, -0.0137, -0.0224,  ...,  0.0226, -0.0088, -0.0087],\n",
       "        [-0.0106,  0.0319,  0.0057,  ..., -0.0124, -0.0169, -0.0078]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.expr_encoder.encoder[4].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjkobject\u001b[0m (\u001b[33mml4ig\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../data/tensorboard/wandb/run-20241205_171025-0g3z31yt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml4ig/scprint_test/runs/0g3z31yt' target=\"_blank\">clean-dragon-11</a></strong> to <a href='https://wandb.ai/ml4ig/scprint_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml4ig/scprint_test' target=\"_blank\">https://wandb.ai/ml4ig/scprint_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml4ig/scprint_test/runs/0g3z31yt' target=\"_blank\">https://wandb.ai/ml4ig/scprint_test/runs/0g3z31yt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    }
   ],
   "source": [
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"scprint_test\",\n",
    "                           save_dir=\"../data/tensorboard\")\n",
    "wandb_logger.watch(model, log='all', log_freq=50, log_graph=True)\n",
    "\n",
    "# tlogger = TensorBoardLogger(save_dir=\"../data/tensorboard\")\n",
    "# tlogger.log_graph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    }
   ],
   "source": [
    "chckp = ModelCheckpoint(monitor=\"val_loss\", save_top_k=-1)\n",
    "trainingmode = TrainingMode(\n",
    "    do_denoise=True,\n",
    "    noise=[0.6],\n",
    "    do_mvc=False,\n",
    "    do_adv_cls=False,\n",
    "    run_full_forward=True,\n",
    "    mask_ratio=[\"TF\"],\n",
    "    warmup_duration=100,\n",
    "    fused_adam=False,\n",
    "    lr_reduce_patience=200,\n",
    ")\n",
    "trainer = Trainer(precision=\"16-mixed\", gradient_clip_val=30, gradient_clip_algorithm=\"norm\", max_time={\"hours\": 2}, limit_val_batches=1, callbacks=[\n",
    "                  trainingmode], accumulate_grad_batches=1, check_val_every_n_epoch=1, reload_dataloaders_every_n_epochs=1000000, \n",
    "                  #logger=wandb_logger\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                            | Type                         | Params | Mode \n",
      "------------------------------------------------------------------------------------------\n",
      "0  | gene_encoder                    | GeneEncoder                  | 5.9 M  | train\n",
      "1  | expr_encoder                    | ContinuousValueEncoder       | 66.8 K | train\n",
      "2  | pos_encoder                     | PositionalEncoding           | 0      | train\n",
      "3  | class_encoder                   | CategoryValueEncoder         | 2.6 K  | train\n",
      "4  | depth_encoder                   | ContinuousValueEncoder       | 66.8 K | train\n",
      "5  | transformer                     | FlashTransformer             | 11.6 M | train\n",
      "6  | cell_transformer                | FlashTransformer             | 8.7 M  | train\n",
      "7  | expr_decoder                    | ExprDecoder                  | 133 K  | train\n",
      "8  | cls_decoders                    | ModuleDict                   | 315 K  | train\n",
      "9  | grad_reverse_discriminator_loss | AdversarialDiscriminatorLoss | 134 K  | train\n",
      "10 | mvc_decoder                     | MVCDecoder                   | 262 K  | train\n",
      "------------------------------------------------------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "5.9 M     Non-trainable params\n",
      "27.2 M    Total params\n",
      "108.718   Total estimated model params size (MB)\n",
      "607       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_fit_start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ca07be8fd24972a20da69cf6cc8cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ml4ig1/Documents code/scDataLoader/scdataloader/datamodule.py\u001b[0m(466)\u001b[0;36m__iter__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    464 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    465 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 466 \u001b[0;31m        \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    467 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    468 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor([ 94807,  92578, 105883, 371196, 389601, 285637, 192735, 294194, 364952,\n",
      "        267701])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212f858057e24334b917e7110b956acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ml4ig1/Documents code/scPRINT/scprint/model/model.py\u001b[0m(730)\u001b[0;36mtraining_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    728 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    729 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 730 \u001b[0;31m        total_loss, losses = self._full_training(\n",
      "\u001b[0m\u001b[0;32m    731 \u001b[0;31m            \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    732 \u001b[0;31m            \u001b[0mdo_denoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_denoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** AttributeError: 'Linear' object has no attribute 'weights'. Did you mean: 'weight'?\n",
      "\u001b[1;32m    725 \u001b[0m            \u001b[0m_type_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_description_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    726 \u001b[0m        \"\"\"\n",
      "\u001b[1;32m    727 \u001b[0m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    728 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    729 \u001b[0m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 730 \u001b[0;31m        total_loss, losses = self._full_training(\n",
      "\u001b[0m\u001b[1;32m    731 \u001b[0m            \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    732 \u001b[0m            \u001b[0mdo_denoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_denoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    733 \u001b[0m            \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    734 \u001b[0m            \u001b[0mdo_next_tp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_next_tp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    735 \u001b[0m            \u001b[0mdo_cce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_cce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0382,  0.0023, -0.0086,  ...,  0.0048,  0.0066, -0.0260],\n",
      "        [-0.0003, -0.0039,  0.0257,  ...,  0.0108, -0.0125,  0.0233],\n",
      "        [-0.0002,  0.0473,  0.0204,  ..., -0.0133,  0.0038,  0.0198],\n",
      "        ...,\n",
      "        [ 0.0330,  0.0147, -0.0138,  ...,  0.0136,  0.0110,  0.0110],\n",
      "        [ 0.0221, -0.0137, -0.0224,  ...,  0.0226, -0.0088, -0.0087],\n",
      "        [-0.0106,  0.0319,  0.0057,  ..., -0.0124, -0.0169, -0.0078]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n"
     ]
    }
   ],
   "source": [
    "trainingmode = TrainingMode(\n",
    "    do_denoise=True,\n",
    "    noise=[0.6],\n",
    "    do_cce=True,\n",
    "    cce_sim=0.6,\n",
    "    do_ecs=False,\n",
    "    ecs_threshold=0.4,\n",
    "    ecs_scale=0.05,\n",
    "    class_scale=0.08,\n",
    "    do_cls=True,\n",
    "    do_mvc=False,\n",
    "    do_adv_cls=False,\n",
    "    run_full_forward=True,\n",
    "    do_next_tp=False,\n",
    "    mask_ratio=[\"TF\"],\n",
    "    fused_adam=False,\n",
    "    lr_reduce_monitor=None,\n",
    "    \n",
    ")\n",
    "overfit_trainer = Trainer(precision=\"16-mixed\", gradient_clip_val=10, max_time={\"hours\": 2}, limit_val_batches=1, callbacks=[\n",
    "                  trainingmode], accumulate_grad_batches=1, check_val_every_n_epoch=10_000, overfit_batches=1, \n",
    "                  reload_dataloaders_every_n_epochs=1_000_000, num_sanity_val_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'overfit_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moverfit_trainer\u001b[49m\u001b[38;5;241m.\u001b[39mfit(model, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'overfit_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "overfit_trainer.fit(model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scprint2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
